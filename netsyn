#!/usr/bin/env python3

##########
# Import #
##########

import argparse
import os
import shutil
import logging
import re
import sys
import filecmp
import glob

import yaml
import GetINSDCFiles, ClusteringIntoFamilies, SyntenyFinder, DataExport
import common

#############
# Functions #
#############
def checkArguments(parser):
    '''
    Arguments checking.
    '''
    args = parser.parse_args()

    if not args.UniProtACList:
        if not args.CorrespondingFile:
                parser.error('missing option: please specify the input option (--UniProtACList and/or --CorrespondingFile).')
        if args.conserveDownloadedINSDC:
            parser.error('ambigous option: --conserveDownloadedINSDC option require of --UniProtACList option. Please specify the --UniProtACList option.')

    if not args.RedundancyRemoval == 'FALSE':
        if args.RedundancyRemovalLabel == 'FALSE' and args.RedundancyRemovalTaxonomy == 'FALSE':
            parser.error('Missing RedundancyRemovalLabel or RedundancyRemovalTaxonomy')

    if not args.RedundancyRemovalLabel == 'FALSE' and not args.RedundancyRemovalTaxonomy == 'FALSE':
        parser.error('RedundancyRemovalLabel or RedundancyRemovalTaxonomy are incompatible')

    if not args.RedundancyRemovalLabel == 'FALSE' or not args.RedundancyRemovalTaxonomy == 'FALSE':
        args.RedundancyRemoval = 'TRUE'

    if not args.RedundancyRemovalTaxonomy == 'FALSE':
        args.RedundancyRemovalTaxonomy = '&'+args.RedundancyRemovalTaxonomy

    return args

def argumentsParser():
    '''
    Arguments parsing.
    '''
    parser = argparse.ArgumentParser(description='version: {}'.format(common.global_dict['version']),
                                     usage='netsyn -u <UniProtAC.list> -o <OutputDirName> [-md <metadataFile>, --conserveDownloadedINSDC]\n\
       netsyn -u <UniProtAC.list> -c <CorrespondingFileName> -o <OutputDirName> [-md <metadataFile>, --conserveDownloadedINSDC]\n\
       netsyn -c <CorrespondingFileName> -o <OutputDirName> [-md <metadataFile>]',
                                     formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument('-v', '--version', action='version', version=common.global_dict['version'])

    group1 = parser.add_argument_group('General settings')
    group1.add_argument('-u', '--UniProtACList', type=str,
                        help='UniProt accession list input(cf: wiki).')
    group1.add_argument('-c', '--CorrespondingFile', type=str,
                        help='Input file of corresponding between: protein_AC/nucleic_AC/nucleic_File_Path (cf: wiki).')
    group1.add_argument('-o', '--OutputDirName', type=str, required=True,
                        help='Output directory name, used for define the project name.')
    group1.add_argument('-md', '--MetaDataFile', type=str,
                        help='File containing the metadata.')
    group1.add_argument('-pd', '--ProjectDescription', type=str,
                        default='No description', help='The project description.\nDefault value: No description.')
    group1.add_argument('-np', '--newProject', action='store_true',
                        help='Force the creation of a new projet. Overwrites the project of the same name.')
    group1.add_argument('--conserveDownloadedINSDC', action='store_true',
                        help='Conserve the dowlowded files from GetINSDC. Requieres of --UniProtACList option.')

    group4 = parser.add_argument_group('Clustering settings')
    group4.add_argument('-mc', '--MinCoverage', type=float,
                        default=0.8, help='Minimal coverage allowed.\nDefault value: 0.8.')
    group4.add_argument('-id', '--Ident', type=float,
                        default=0.3, help='Sequence identity.\nDefault value: 0.3.')

    group3 = parser.add_argument_group('Synteny settings')
    group3.add_argument('-ws', '--WindowSize', type=int,
                        default=common.global_dict['maxGCSize'],
                        help='Window size of genomic contexts to compare (target gene inclued).\nDefault value: {}.'.format(common.global_dict['maxGCSize']),
                        choices=common.widowsSizePossibilities(common.global_dict['minGCSize'],common.global_dict['maxGCSize']))
    group3.add_argument('-sg', '--SyntenyGap', type=int, default=3,
                        help='Number of genes allowed betwenn tow genes in synteny.\nDefault value: 3.')
    group3.add_argument('-st', '--ScoreType', type=int,
                        default=1, choices=[1,2], help='Default score = 1 or Weighted score = 2')
    group3.add_argument('-sf', '--SyntenyFilter', type=str, default='off',
                        choices=['on','off'],  help='Keep synteny near target genes.\nDefault value: off.')
    group3.add_argument('-ssc', '--SyntenyScoreCuttoff', type=float,
                        default=0, help='Define the Synteny Score Cuttoff to conserved.\nDefault value: >= 0.')

    group6 = parser.add_argument_group('Graph Analysis settings')
    group6.add_argument('-cm', '--ClusteringMethod', type=str,
                        choices=['Multiple','MCL','EdgeBetweeness','Louvain','WalkTrap'],
                        default='MCL', help='Clustering method choose in : Multiple (All), MCL (small graph), EdgeBetweeness (medium graph), Louvain (medium graph) or WalkTrap (big  graph).\nDefault value: MCL')
    group6.add_argument('-rr', '--RedundancyRemoval', type=str,
                        default='FALSE', choices=['FALSE','TRUE'], help='Should the pipeline compute a graph contracting same cluster same label.\nDefault value: FALSE.')
    group6.add_argument('-rrl', '--RedundancyRemovalLabel', type=str,
                        default='FALSE', help='Label of the column on which the redundancy will be computed (Only with --RedundancyRemoval TRUE)')
    group6.add_argument('-rrt', '--RedundancyRemovalTaxonomy', type=str, choices=['superkingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'],
                        default='FALSE', help='Taxonomic rank on which the redundancy will be computed (Only with --RedundancyRemoval TRUE)')

    group7 = parser.add_argument_group('logger')
    group7.add_argument( '--log_level',
                         type=str,
                         nargs='?',
                         default='INFO',
                         help='log level',
                         choices=['ERROR', 'error', 'WARNING', 'warning', 'INFO', 'info', 'DEBUG', 'debug'],
                         required=False )
    group7.add_argument( '--log_file',
                         type=str,
                         nargs='?',
                         help='log file (use the stderr by default). Disable the log colors.',
                         required=False )

    args = checkArguments(parser)
    return(args)

def writeCurrentVersion(version, fileName):
    '''
    Saved the current version in a file.
    '''
    with open(fileName, 'w') as file:
        file.write(version)

def getLatestVersionUsed(fileName):
    '''
    Return the latest version used in the previous analysis.
    '''
    if os.path.isfile(fileName):
        with open(fileName, 'r') as file:
            version = file.read()
    else:
        version = common.global_dict['version']
    return version

def checkSameVersion(fileName):
    '''
    Exit if the latest version and the current version are different.
    '''
    logger = logging.getLogger(checkSameVersion.__name__)
    currentVersion = common.global_dict['version']
    latestVersion = getLatestVersionUsed(fileName)
    if not currentVersion == latestVersion:
        logger.error('The previous version used ({}) are different of the current version ({}).Please, create a new project or use -np option.'.format(latestVersion,currentVersion))
        exit(1)

def orderDefinition(uniprotACList, correspondingFile):
    '''
    Setting the code execution order.
    '''
    # logger = logging.getLogger(orderDefinition.__name__)
    if correspondingFile and not uniprotACList:
        order = [
            common.global_dict['boxName']['ClusteringIntoFamilies'],
            common.global_dict['boxName']['SyntenyFinder'],
            common.global_dict['boxName']['DataExport']
        ]
    else:
        order = [
            common.global_dict['boxName']['GetINSDCFiles'],
            common.global_dict['boxName']['ClusteringIntoFamilies'],
            common.global_dict['boxName']['SyntenyFinder'],
            common.global_dict['boxName']['DataExport']
        ]
    # logger.debug('Execution order: {}'.format(order))
    return order

def saveInputFiles(inputI, inputII):
    '''
    Saves the input files.
    '''
    if inputI:
        shutil.copyfile(inputI, common.global_dict['uniprotACListSaved'])
    if inputII:
        shutil.copyfile(inputII, common.global_dict['correspondingFileSaved'])


def createYamlSettingsFile(args):
    '''
    YAML settings file creation. Used for resumption of analysis.
    '''
    global ORDERBOX
    YAML_File = {}
    gap = 1 if args.CorrespondingFile else 0 # Depending on whether execution start from GetINSDCFiles or ClusteringIntoFamilies
    with open(common.global_dict['settingsFileName'], 'w') as outfile:
        YAML_File['General informations'] = {}
        YAML_File['General informations']['UniProtACList'] = args.UniProtACList
        YAML_File['General informations']['CorrespondingFile'] = args.CorrespondingFile
        YAML_File['General informations']['OutputDirName'] = args.OutputDirName
        YAML_File['General informations']['ProjectDescription'] = args.ProjectDescription
        YAML_File['General informations']['MetaDataFile'] = args.MetaDataFile
        YAML_File = writtingYAML(YAML_File, outfile, 'General informations')

        YAML_File[ORDERBOX[1-gap]] = {}
        YAML_File[ORDERBOX[1-gap]]['MinCoverage'] = args.MinCoverage
        YAML_File[ORDERBOX[1-gap]]['Ident'] = args.Ident
        YAML_File = writtingYAML(YAML_File, outfile, ORDERBOX[1-gap])

        YAML_File[ORDERBOX[2-gap]] = {}
        YAML_File[ORDERBOX[2-gap]]['WindowSize'] = args.WindowSize
        YAML_File[ORDERBOX[2-gap]]['SyntenyGap'] = args.SyntenyGap
        YAML_File[ORDERBOX[2-gap]]['SyntenyFilter'] = args.SyntenyFilter
        YAML_File = writtingYAML(YAML_File, outfile, ORDERBOX[2-gap])

        YAML_File[ORDERBOX[3-gap]] = {}
        YAML_File[ORDERBOX[3-gap]]['ScoreType'] = args.ScoreType
        YAML_File[ORDERBOX[3-gap]]['SyntenyScoreCuttoff'] = args.SyntenyScoreCuttoff
        YAML_File[ORDERBOX[3-gap]]['ClusteringMethod'] = args.ClusteringMethod
        YAML_File[ORDERBOX[3-gap]]['RedundancyRemoval'] = args.RedundancyRemoval
        YAML_File[ORDERBOX[3-gap]]['RedundancyRemovalLabel'] = args.RedundancyRemovalLabel
        YAML_File[ORDERBOX[3-gap]]['RedundancyRemovalTaxonomy'] = args.RedundancyRemovalTaxonomy
        YAML_File = writtingYAML(YAML_File, outfile, ORDERBOX[3-gap])

def writtingYAML(YAML_to_write, outfile, block):
    '''
    YAML file writting.
    '''
    stream = yaml.dump(YAML_to_write, default_flow_style=False)
    if block == 'General informations':
        outfile.write('# ~~~~~ {}\n'.format('General informations'))
    elif block == ORDERBOX[1]:
        outfile.write('# ~~~~~ {}\n'.format(ORDERBOX[1]))
    else:
        outfile.write('# ~~~~~ Parameters for ' + block + '\n')
    outfile.write(stream)
    outfile.write('\n')
    return({})

def getBoxToResum(args):
    '''
    detection of settings modification
    make the association with box
    '''
    with open(common.global_dict['settingsFileName'], 'r') as file:
        oldSettings = yaml.load(file)
    newsettings = vars(args)
    for boxKey in oldSettings:
        for parameterKey in oldSettings[boxKey]:
            if not newsettings[parameterKey] == oldSettings[boxKey][parameterKey]:
                logger.debug('New settings detected')
                return boxKey
    return None

def getLastBoxDone():
    '''
    Get name box executed.
    '''
    global ORDERBOX
    logger = logging.getLogger(getLastBoxDone.__name__)
    if os.path.isfile(common.global_dict['reportFileName']):
        with open(common.global_dict['reportFileName'], 'r') as file:
            nameBox = file.read()
        if not nameBox in ORDERBOX:
            logger.critical('File of report wrong.')
            exit(1)
        return nameBox
    return None

def inputFileModificationChecking(newInputFile, oldInputFile):
    '''
    Check that the input file has not been modified.
    '''
    logger = logging.getLogger(inputFileModificationChecking.__name__)
    if not filecmp.cmp(newInputFile,oldInputFile):
        logger.error('The input file {} has been modified. Please start a new project.'.format(newInputFile))
        exit(1)

def inputFilesModificationChecking(inputI, inputII):
    '''
    Check that the input files has not been modified.
    '''
    if inputI:
        inputFileModificationChecking(inputI, common.global_dict['uniprotACListSaved'])
    if inputII:
        inputFileModificationChecking(inputII, common.global_dict['correspondingFileSaved'])

def resumptionFrom(nameBoxToResum, nameBoxDone):
    '''
    Returns the name of the box from which to resume the analysis.
    '''
    global ORDERBOX
    logger = logging.getLogger(resumptionFrom.__name__)
    logger.debug(ORDERBOX)
    logger.debug('{} ,{}'.format(nameBoxToResum, nameBoxDone))
    if nameBoxDone:
        indexBoxDone = ORDERBOX.index(nameBoxDone)
    else:
        indexBoxDone = -1
    if nameBoxToResum:
        indexBoxToResum = ORDERBOX.index(nameBoxToResum)
    else:
        indexBoxToResum = -1
    if indexBoxDone == (len(ORDERBOX)-1):
        ''' Last analysis completed '''
        logger.debug('Last analysis completed')
        if indexBoxToResum == -1:
            ''' Not new analysis '''
            logger.debug('Not new analysis')
            return None
        else:
            ''' New Analysis '''
            logger.debug('New Analysis')
            return ORDERBOX[indexBoxToResum]
    else:
        ''' Last analysis not completed '''
        logger.debug('Last analysis not completed')
        if indexBoxToResum >= 0:
            ''' New Analysis '''
            logger.debug('New Analysis')
            if indexBoxToResum <= indexBoxDone+1:
                ''' New Analysis '''
                logger.debug('New Analysis')
                return ORDERBOX[indexBoxToResum]
            else:
                ''' Error Recovery '''
                logger.debug('Error Recovery')
                return ORDERBOX[indexBoxDone+1]
        else:
            ''' Error Recovery '''
            logger.debug('Error Recovery')
            return ORDERBOX[indexBoxDone+1]

def removeDownloadedINSDC():
    '''
    Remove dolowded INSDC files from GetINSDC step.
    '''
    boxName = common.global_dict['boxName']['GetINSDCFiles']
    dataDirectoryProcess = '{}/{}'.format(common.global_dict['dataDirectory'], boxName)
    filesExtension = common.global_dict['filesExtension']
    filesToRemoved = glob.glob('{}/*.{}'.format(dataDirectoryProcess, filesExtension))
    for file in filesToRemoved:
        os.remove(file)

## Swiss functions : to move into library ?!?
def boxesManager(runFrom, resultsDirectory, analysisNumber):
    '''
    Allows execution from a box.
    '''
    global ORDERBOX
    logger = logging.getLogger(boxesManager.__name__)
    boxesToRun = [ORDERBOX[i] for i in range(ORDERBOX.index(runFrom), len(ORDERBOX))]
    logger.debug('Boxes to run: {}'.format(boxesToRun))
    for nameBox in boxesToRun:
        runBox(nameBox, resultsDirectory, analysisNumber)
        checkingAfterRun(nameBox, resultsDirectory, analysisNumber)
        updateLastBoxDone(nameBox)

def updateLastBoxDone(nameBox):
    '''
    Updating report file.
    '''
    with open(common.global_dict['reportFileName'], 'w') as file:
        file.write(nameBox)

def getAnalysisNumber(outputDirName):
    '''
    Defining of the number of analisys.
    Defining of results directory name for the current analysis.
    '''
    number = 0
    string = r'{}_Results_[0-9]+$'.format(outputDirName)
    existingAnalyzes = [f for f in os.listdir(common.global_dict['workingDirectory']) if re.search(string, f)]
    if existingAnalyzes:
        number = max([int(i.split('_')[-1]) for i in existingAnalyzes])
    analysisNumber = (number + 1)
    resultsDirectory = '{}/{}_Results_{}'.format(common.global_dict['workingDirectory'], outputDirName, analysisNumber)
    return analysisNumber, resultsDirectory

def checkingAfterRun(boxName, resultsDirectory, analysisNumber):
    '''
    Checkpoints before executing box.
    '''
    logger = logging.getLogger(checkingAfterRun.__name__)
    pointsToCheck = common.global_dict['files']
    if not boxName in pointsToCheck.keys():
        logger.critical('No checkpoints for {}'.format(boxName))
        exit(1)
    if pointsToCheck[boxName]:
        error = False
        for fileName in pointsToCheck[boxName].keys():
            error = common.checkFilledFile(pointsToCheck[boxName][fileName], error)
        if error:
            logger.error('Checkpoints not validated after executing of {}.'.format(boxName))
            exit(1)
    else:
        logger.error('Checkpoints is None for {}'.format(boxName))
        exit(1)

def mergeInputsII(outputStepI, inputII, inputMergedName):
    '''
    Merges the inputI and inputII.
    Copies of insdc files of inputII into the process directory of etINSDC step.
    '''
    authorized_columns = common.definesAuthorizedColumns()
    mandatory_columns = common.definesMandatoryColumns()

    rowsI = common.parseInputII(outputStepI, authorized_columns, mandatory_columns)
    rowsII = common.parseInputII(inputII, authorized_columns, mandatory_columns)

    errors = False
    accessions = []
    for row in rowsI:
        accessions.append(row['protein_AC'])
        if 'taxon_ID' in rowsII[0].keys():
            row['taxon_ID'] = 'NA'
    for row in rowsII:
        if row['protein_AC'] in accessions:
            logger.error('{}: Entry duplicated!!!!!!!!!!!!!!!!!!blabla de la provenance de la duplication.'.format(row['protein_AC']))
            errors = True
        else:
            accessions.append(row['protein_AC'])
            if not common.global_dict['inputIheader'] in row.keys():
                row[common.global_dict['inputIheader']] = 'NA'
            rowsI.append(row)
    if errors:
        logger.error('Merge of inputs failed.')
        exit(1)
    else:
        with open(inputMergedName, 'w') as file:
            firstLine = True
            for row in rowsI:
                if firstLine:
                    headers = [header for header in row.keys()]
                    file.write('{}\n'.format('\t'.join(headers)))
                    firstLine = False
                line = ['' for i in row.values()]
                for header, value in row.items():
                    line[headers.index(header)] = value
                file.write('{}\n'.format('\t'.join(line)))
    return

def runBox(nameBox, resultsDirectory, analysisNumber):
    '''
    Running box.
    '''
    global args
    logger = logging.getLogger(runBox.__name__)
    if nameBox == 'GetINSDCFiles':
        logger = logging.getLogger('runGetINSDCFiles')
        GetINSDCFiles.run(args.UniProtACList)
    elif nameBox == 'ClusteringIntoFamilies':
        logger = logging.getLogger('runClusteringIntoFamilies')
        if args.UniProtACList and args.CorrespondingFile:
            mergeInputsII(common.global_dict['files']['GetINSDCFiles']['inputClusteringStep'],
                                       args.CorrespondingFile,
                                       common.global_dict['inputsMergedName'])
            inputBoxII = common.global_dict['inputsMergedName']
        elif args.CorrespondingFile and not args.UniProtACList:
            inputBoxII = args.CorrespondingFile
        else:
            inputBoxII = common.global_dict['files']['GetINSDCFiles']['inputClusteringStep']
        ClusteringIntoFamilies.run(inputBoxII, args.Ident, args.MinCoverage)
        if not args.conserveDownloadedINSDC:
            logger.info('Removing downloded files in GetINSDC step.')
            removeDownloadedINSDC()
        else:
            logger.info('Conserving downloded files in GetINSDC step.')
    elif nameBox == 'SyntenyFinder':
        logger = logging.getLogger('runSyntenyFinder')
        genomicContexts = common.global_dict['files']['ClusteringIntoFamilies']['genomicContexts']
        targets_list = common.global_dict['files']['ClusteringIntoFamilies']['targets']
        SyntenyFinder.run(genomicContexts, targets_list, args.WindowSize, args.SyntenyGap)
    elif nameBox == 'DataExport':
        logger = logging.getLogger('runDataExport')
        nodes = common.global_dict['files']['SyntenyFinder']['nodes']
        edges = common.global_dict['files']['SyntenyFinder']['edges']
        taxonomy = common.global_dict['files']['ClusteringIntoFamilies']['lineage']
        contigs_info = common.global_dict['files']['ClusteringIntoFamilies']['contigs']
        metadata = args.MetaDataFile #'BKACE_id.dat' #### chemain en dur !!!! #######################   *******************    ######################
        DataExport.run(nodes, edges, taxonomy, contigs_info, metadata, resultsDirectory)
    else:
        logger.critical('No execution planned for {}'.format(runBox))
        exit(1)
####################
# Script execution #
####################

if __name__ == '__main__':
    ######################
    # Parse command line #
    ######################
    args = argumentsParser()
    ##########
    # Logger #
    ##########
    common.parametersLogger(args)
    logger = logging.getLogger('main')
    #########################
    # Dependancies checking #
    #########################
    common.dependanciesChecking()
    ##############
    # Constantes #
    ##############
    common.constantsInitialization(args.OutputDirName, args.UniProtACList, args.CorrespondingFile)
    ORDERBOX = orderDefinition(args.UniProtACList, args.CorrespondingFile)
    logger.debug('ORDERBOX: {}'.format(ORDERBOX))
    ############
    # Switches #
    ############
    if args.newProject and os.path.isdir(common.global_dict['workingDirectory']):
        shutil.rmtree(common.global_dict['workingDirectory'])
    checkSameVersion(common.global_dict['versionFileName'])
    if not os.path.isfile(common.global_dict['settingsFileName']):
        if os.path.isdir(common.global_dict['workingDirectory']):
            logger.error('Project name already used. Please change your project name or use --newProject.')
            exit(1)
        '''Pas de dernier setting == premiere analyse'''
        logger.info('New NetSyn project: {}'.format(common.global_dict['workingDirectory']))
        os.mkdir(common.global_dict['workingDirectory'])
        os.mkdir(common.global_dict['dataDirectory'])
        analysisNumber, resultsDirectory = getAnalysisNumber(args.OutputDirName)
        common.filesNameInitialization(resultsDirectory, args.OutputDirName, analysisNumber)
        saveInputFiles(args.UniProtACList, args.CorrespondingFile)
        runFromBox = ORDERBOX[0]
        nameBoxToResum = None
        nameBoxDone = None
    else:
        '''Fichier de setting == analyse deja lance'''
        inputFilesModificationChecking(args.UniProtACList, args.CorrespondingFile)
        nameBoxToResum = getBoxToResum(args)
        if nameBoxToResum == 'General informations':
            logger.error('Already existing project. Please start a new project.')
            exit(1)
        else:
            logger.info('NetSyn project recovery: {}'.format(common.global_dict['workingDirectory']))
        nameBoxDone = getLastBoxDone()
        runFromBox = resumptionFrom(nameBoxToResum, nameBoxDone)
        analysisNumber, resultsDirectory = getAnalysisNumber(args.OutputDirName)
        common.filesNameInitialization(resultsDirectory, args.OutputDirName, analysisNumber)
    createYamlSettingsFile(args)
    if runFromBox:
        '''Execution a partir de la boite x'''
        runFromBoxIndex = ORDERBOX.index(runFromBox)
        updateLastBoxDone(ORDERBOX[runFromBoxIndex-1] if runFromBoxIndex-1 >= 0 else '')
        logger.debug('Box from which resume: {}'.format(nameBoxToResum))
        logger.debug('Last box done: {}'.format(nameBoxDone))
        logger.debug('Run from: {}'.format(runFromBox))
        writeCurrentVersion(common.global_dict['version'], common.global_dict['versionFileName'])
        boxesManager(runFromBox, resultsDirectory, analysisNumber) # Fonction de fin d'execution d'une box
    else:
        '''Nous avons deja des resultats'''
        logger.debug('Box from which resume: {}'.format(nameBoxToResum))
        logger.debug('Last box done: {}'.format(nameBoxDone))
        logger.debug('Run from: {}'.format(runFromBox))
        logger.info('This analysis already existing.')
